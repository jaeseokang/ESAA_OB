{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ëª¨ë¸ í›ˆë ¨ ì—°ìŠµ ë¬¸ì œ**\n",
        "___\n",
        "- ì¶œì²˜ : í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹ Ch04 ì—°ìŠµë¬¸ì œ 1, 5, 9, 10\n",
        "- ê°œë… ë¬¸ì œì˜ ê²½ìš° í…ìŠ¤íŠ¸ ì…€ì„ ì¶”ê°€í•˜ì—¬ ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. ìˆ˜ë°±ë§Œ ê°œì˜ íŠ¹ì„±ì„ ê°€ì§„ í›ˆë ¨ ì„¸íŠ¸ì—ì„œëŠ” ì–´ë–¤ ì„ í˜• íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?**\n",
        "___\n"
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•, ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•"
      ],
      "metadata": {
        "id": "0-4Q6J9BRQ1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•˜ê³  ì—í¬í¬ë§ˆë‹¤ ê²€ì¦ ì˜¤ì°¨ë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë´¤ìŠµë‹ˆë‹¤. ê²€ì¦ ì˜¤ì°¨ê°€ ì¼ì •í•˜ê²Œ ìƒìŠ¹ë˜ê³  ìˆë‹¤ë©´ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ” ê±¸ê¹Œìš”? ì´ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‚˜ìš”?**\n",
        "___"
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í•™ìŠµë¥ ì„ ì¡°ì ˆí•´, ì•Œê³ ë¦¬ì¦˜ì´ ìµœì ì ì— ë„ë‹¬í•˜ë„ë¡ í•œë‹¤. ë°˜ë³µ íšŸìˆ˜ë¥¼ í¬ê²Œ ì§€ì •í•˜ê³  ê·¸ë ˆì´ë””ì–¸íŠ¸ ë²¡í„°ê°€ ì‘ì•„ì§€ë©´, ì¦‰ ë²¡í„°ì˜ ë…¸ë¦„ì´ ì–´ë–¤ ê°’ Îµ ë³´ë‹¤ ì‘ì•„ì§€ë©´ ê²½ì‚¬ í•˜ê°•ë²•ì´ ê±°ì˜ ìµœì†Ÿê°’ì— ë„ë‹¬í•œ ê²ƒì´ë¯€ë¡œ ì•Œê³ ë¦¬ì¦˜ì„ ì¤‘ì§€í•œë‹¤.\n",
        "- ì¡°ê¸° ì¢…ë£Œ\n",
        "- ë¦¿ì§€, ë¼ì˜ ë“± ê·œì œ ì ìš©"
      ],
      "metadata": {
        "id": "bwwGpOgjRn6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. ë¦¿ì§€ íšŒê·€ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ í›ˆë ¨ ì˜¤ì°¨ê°€ ê²€ì¦ ì˜¤ì°¨ê°€ ê±°ì˜ ë¹„ìŠ·í•˜ê³  ë‘˜ ë‹¤ ë†’ì•˜ìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì—ëŠ” ë†’ì€ í¸í–¥ì´ ë¬¸ì œì¸ê°€ìš”, ì•„ë‹ˆë©´ ë†’ì€ ë¶„ì‚°ì´ ë¬¸ì œì¸ê°€ìš”? ê·œì œ í•˜ì´í¼íŒŒë¼ë¯¸í„° $\\alpha$ë¥¼ ì¦ê°€ì‹œì¼œì•¼ í• ê¹Œìš” ì•„ë‹ˆë©´ ì¤„ì—¬ì•¼ í• ê¹Œìš”?**\n",
        "___"
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê³¼ì†Œì í•©ì´ ë°œìƒí•œ ìƒí™©ìœ¼ë¡œ, ë†’ì€ í¸í–¥ ë¬¸ì œë¡œ, ê·œì œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ğ›¼ë¥¼ ì¤„ì—¬ì•¼ í•¨."
      ],
      "metadata": {
        "id": "9Xg66ManS-e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ì´ìœ ëŠ”?**\n",
        "___\n",
        "- í‰ë²”í•œ ì„ í˜• íšŒê·€(ì¦‰, ì•„ë¬´ëŸ° ê·œì œê°€ ì—†ëŠ” ëª¨ë¸) ëŒ€ì‹  ë¦¿ì§€ íšŒê·€\n",
        "- ë¦¿ì§€ íšŒê·€ ëŒ€ì‹  ë¼ì˜ íšŒê·€\n",
        "- ë¼ì˜ íšŒê·€ ëŒ€ì‹  ì—˜ë¼ìŠ¤í‹±ë„·"
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ê·œì œ ì—†ëŠ” ì„ í˜• íšŒê·€ ëŒ€ì‹  ë¦¿ì§€ íšŒê·€ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°€ëŠ¥í•œ í•œ ì‘ê²Œ ìœ ì§€í•˜ê²Œ í•˜ê¸° ìœ„í•¨.\n",
        "2. ë¦¿ì§€ íšŒê·€ ëŒ€ì‹  ë¼ì˜ íšŒê·€ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ëœ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ì œê±°í•˜ê¸° ìœ„í•¨.\n",
        "3. ë¼ì˜ íšŒê·€ ëŒ€ì‹  ì—˜ë¼ìŠ¤í‹±ë„·ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë¦¿ì§€ íšŒê·€ì™€ ë¼ì˜ íšŒê·€ë¥¼ ì ˆì¶©í•œ ëª¨ë¸ë¡œ, í˜¼í•© ë¹„ìœ¨ rì„ ì‚¬ìš©í•´ ì¡°ì ˆí•˜ê¸° ìœ„í•¨. (ë‹¤ì¤‘ê³µì„ ì„±ì— ë” ê°•í•œ ëª¨ìŠµì„ ë³´ì„)"
      ],
      "metadata": {
        "id": "XNU4iZJyTMBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì¶”ê°€) ì¡°ê¸° ì¢…ë£Œë¥¼ ì‚¬ìš©í•œ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ iris ë°ì´í„°ë¥¼ í™œìš©í•´ ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”(ì‚¬ì´í‚·ëŸ°ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”)**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QIZpOEYJVIAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]\n",
        "y = iris[\"target\"]\n",
        "\n",
        "# bias ì¶”ê°€\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
        "\n",
        "np.random.seed(2042)\n",
        "\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "# ì˜¬ë°”ë¥¸ ì¸ë±ìŠ¤ ë¶„í• \n",
        "train_idx = rnd_indices[:train_size]\n",
        "valid_idx = rnd_indices[train_size:train_size + validation_size]\n",
        "test_idx = rnd_indices[train_size + validation_size:]\n",
        "\n",
        "X_train = X_with_bias[train_idx]\n",
        "y_train = y[train_idx]\n",
        "\n",
        "X_valid = X_with_bias[valid_idx]\n",
        "y_valid = y[valid_idx]\n",
        "\n",
        "X_test = X_with_bias[test_idx]\n",
        "y_test = y[test_idx]\n",
        "\n",
        "# ì†Œí”„íŠ¸ë§¥ìŠ¤ (ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •í™”)\n",
        "def softmax(logits):\n",
        "    # logits: (n_samples, n_classes)\n",
        "    shifted = logits - np.max(logits, axis=1, keepdims=True)   # ì•ˆì •ì„± ìœ„í•´ ìµœëŒ€ê°’ ë¹¼ê¸°\n",
        "    exps = np.exp(shifted)\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "# ì›-í•« ì¸ì½”ë”©\n",
        "def one_hot(y, n_classes):\n",
        "    return np.eye(n_classes)[y]\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "eta = 0.1\n",
        "n_iterations = 5000\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1   # L2 ê·œì œ ê°•ë„\n",
        "best_loss = np.inf\n",
        "\n",
        "n_inputs = X_train.shape[1]\n",
        "n_outputs = len(np.unique(y_train))\n",
        "\n",
        "Y_train_oh = one_hot(y_train, n_outputs)\n",
        "Y_valid_oh = one_hot(y_valid, n_outputs)\n",
        "\n",
        "# íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n",
        "Theta = np.random.randn(n_inputs, n_outputs) * 0.01\n",
        "\n",
        "# ì¡°ê¸° ì¢…ë£Œ ì„¤ì • (patience ì‚¬ìš©)\n",
        "patience = 10\n",
        "checks_without_progress = 0\n",
        "best_theta = None\n",
        "\n",
        "m = len(X_train)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²• (ì „ì²´ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©)\n",
        "    logits = X_train.dot(Theta)            # (m, n_outputs)\n",
        "    Y_proba = softmax(logits)              # (m, n_outputs)\n",
        "\n",
        "    # gradient: (1/m) * X^T (Y_proba - Y_onehot) + reg\n",
        "    error = Y_proba - Y_train_oh\n",
        "    # ê·œì œëŠ” bias(ì²« í–‰)ì— ëŒ€í•´ì„œëŠ” ì ìš©í•˜ì§€ ì•ŠìŒ\n",
        "    reg_term = (alpha / m) * np.vstack([np.zeros((1, n_outputs)), Theta[1:]])\n",
        "    gradients = (1.0 / m) * X_train.T.dot(error) + reg_term\n",
        "\n",
        "    Theta = Theta - eta * gradients\n",
        "\n",
        "    # ê²€ì¦ ì†ì‹¤ ê³„ì‚° (êµì°¨ ì—”íŠ¸ë¡œí”¼ + L2)\n",
        "    val_logits = X_valid.dot(Theta)\n",
        "    val_proba = softmax(val_logits)\n",
        "    xentropy_loss = -np.mean(np.sum(Y_valid_oh * np.log(val_proba + epsilon), axis=1))\n",
        "    l2_loss = 0.5 * np.sum(np.square(Theta[1:]))   # bias ì œì™¸\n",
        "    loss = xentropy_loss + alpha * l2_loss\n",
        "\n",
        "    if iteration % 100 == 0:\n",
        "        print(f\"iter {iteration:4d}  valid loss = {loss:.6f}\")\n",
        "\n",
        "    # ì¡°ê¸° ì¢…ë£Œ ì²´í¬\n",
        "    if loss < best_loss - 1e-8:\n",
        "        best_loss = loss\n",
        "        best_theta = Theta.copy()\n",
        "        checks_without_progress = 0\n",
        "    else:\n",
        "        checks_without_progress += 1\n",
        "        if checks_without_progress >= patience:\n",
        "            print(f\"ì¡°ê¸° ì¢…ë£Œ: iter={iteration}, best_valid_loss={best_loss:.6f}\")\n",
        "            Theta = best_theta  # ê²€ì¦ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ íŒŒë¼ë¯¸í„°ë¡œ ë³µì›\n",
        "            break\n",
        "\n",
        "# ìµœì¢… í‰ê°€ (í…ŒìŠ¤íŠ¸ ì •í™•ë„)\n",
        "test_logits = X_test.dot(Theta)\n",
        "test_proba = softmax(test_logits)\n",
        "y_pred = np.argmax(test_proba, axis=1)\n",
        "test_acc = np.mean(y_pred == y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lZ4oJBHckBN",
        "outputId": "2e7c274d-af2c-464b-c638-73cd7cc9662e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter    0  valid loss = 1.017289\n",
            "iter  100  valid loss = 0.699565\n",
            "iter  200  valid loss = 0.634423\n",
            "ì¡°ê¸° ì¢…ë£Œ: iter=285, best_valid_loss=0.626053\n",
            "Test accuracy: 0.9333333333333333\n"
          ]
        }
      ]
    }
  ]
}